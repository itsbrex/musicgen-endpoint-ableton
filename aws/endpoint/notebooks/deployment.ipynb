{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment notebook\n",
    "\n",
    "This notebook is used to deploy the endpoint using the Sagemaker SDK, both locally and \n",
    "online. This is not meant to be the main source of endpoint provision, which should be\n",
    "done with terraform through the CD pipeline, but rather this is a way to test that\n",
    "everything works before provisioning it.\n",
    "\n",
    "It also register the model in the model registry for CD provisioning later.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "**Note**: this notebook must be run outside of the `dev environment` container. This is \n",
    "because the sagemaker local development container can't spin up.\n",
    "\n",
    "The development workflow is as following: \n",
    "- All the development happens inside the dev container\n",
    "- Only when there is the need to run the notebook, this is run from another vscode \n",
    "window connected with ssh only\n",
    "- The `inference.py` script should be tested with their invidual functions, eg: as shown\n",
    "in the `aws/endpoint/src/tests/` folder. Once these work as expected, only then the you\n",
    "should execute the notebook. This is a huge time-saver, because the notebook can be\n",
    "very slow to run.\n",
    "\n",
    "---\n",
    "\n",
    "Before running the cells, make sure you login to AWS using either:\n",
    "\n",
    "- `aws configure sso` → for first time login\n",
    "- `aws sso login` → for all subsequent login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings, shared between local and online deployments\n",
    "\n",
    "model_name = \"musicgen\"\n",
    "model_entry_point = \"../src/code/inference.py\"\n",
    "model_data = \"../model/model.tar.gz\"\n",
    "\n",
    "endpoint_name = \"endpoint-musicgen-0001-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local temp folder to avoid /tmp to become full\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root_dir = Path(os.getcwd()).parents[2].resolve()\n",
    "local_temp_folder_path = str(repo_root_dir / \".temp\" / \"sagemaker_local\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker[local]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: arn:aws:iam::138140302683:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AdministratorAccess_6f1d7369dc867f6b\n",
      "Local temp folder path: /home/ubuntu/musicgen-endpoint-ableton/.temp/sagemaker_local\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "session = LocalSession()\n",
    "\n",
    "session.config = {\n",
    "    \"local\": {\n",
    "        \"local_code\": True,\n",
    "        \"container_root\": local_temp_folder_path,\n",
    "    }\n",
    "}\n",
    "\n",
    "session.settings = sagemaker.session_settings.SessionSettings(\n",
    "    local_download_dir = local_temp_folder_path\n",
    ")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"Role:\", role)\n",
    "print(\"Local temp folder path:\", local_temp_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/image.py\", line 862, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/image.py\", line 928, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/image.py\", line 867, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/home/ubuntu/musicgen-endpoint-ableton/.temp/sagemaker_local/tmp0mcgfj8p/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m model_image_uri \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39mimage_uris\u001b[39m.\u001b[39mretrieve(\n\u001b[1;32m      5\u001b[0m     framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     region\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mus-east-1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     instance_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mml.g4dn.xlarge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m PyTorchModel(\n\u001b[1;32m     14\u001b[0m     name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m     15\u001b[0m     role\u001b[39m=\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     sagemaker_session\u001b[39m=\u001b[39msession,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m predictor \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m     23\u001b[0m     initial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m     instance_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlocal_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     endpoint_name\u001b[39m=\u001b[39;49mendpoint_name,\n\u001b[1;32m     26\u001b[0m     sagemaker_session\u001b[39m=\u001b[39;49msession,\n\u001b[1;32m     27\u001b[0m     serializer\u001b[39m=\u001b[39;49mJSONSerializer(),\n\u001b[1;32m     28\u001b[0m     deserializer\u001b[39m=\u001b[39;49mJSONDeserializer(),\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/model.py:1347\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1345\u001b[0m     explainer_config_dict \u001b[39m=\u001b[39m explainer_config\u001b[39m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1347\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mendpoint_from_production_variants(\n\u001b[1;32m   1348\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[1;32m   1349\u001b[0m     production_variants\u001b[39m=\u001b[39;49m[production_variant],\n\u001b[1;32m   1350\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1351\u001b[0m     kms_key\u001b[39m=\u001b[39;49mkms_key,\n\u001b[1;32m   1352\u001b[0m     wait\u001b[39m=\u001b[39;49mwait,\n\u001b[1;32m   1353\u001b[0m     data_capture_config_dict\u001b[39m=\u001b[39;49mdata_capture_config_dict,\n\u001b[1;32m   1354\u001b[0m     explainer_config_dict\u001b[39m=\u001b[39;49mexplainer_config_dict,\n\u001b[1;32m   1355\u001b[0m     async_inference_config_dict\u001b[39m=\u001b[39;49masync_inference_config_dict,\n\u001b[1;32m   1356\u001b[0m )\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1359\u001b[0m     predictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/session.py:4641\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4638\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating endpoint-config with name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, name)\n\u001b[1;32m   4639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint_config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4641\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_endpoint(endpoint_name\u001b[39m=\u001b[39;49mname, config_name\u001b[39m=\u001b[39;49mname, tags\u001b[39m=\u001b[39;49mtags, wait\u001b[39m=\u001b[39;49mwait)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/session.py:4030\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   4027\u001b[0m tags \u001b[39m=\u001b[39m tags \u001b[39mor\u001b[39;00m []\n\u001b[1;32m   4028\u001b[0m tags \u001b[39m=\u001b[39m _append_project_tags(tags)\n\u001b[0;32m-> 4030\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_endpoint(\n\u001b[1;32m   4031\u001b[0m     EndpointName\u001b[39m=\u001b[39;49mendpoint_name, EndpointConfigName\u001b[39m=\u001b[39;49mconfig_name, Tags\u001b[39m=\u001b[39;49mtags\n\u001b[1;32m   4032\u001b[0m )\n\u001b[1;32m   4033\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m   4034\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait_for_endpoint(endpoint_name)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/local_session.py:364\u001b[0m, in \u001b[0;36mLocalSagemakerClient.create_endpoint\u001b[0;34m(self, EndpointName, EndpointConfigName, Tags)\u001b[0m\n\u001b[1;32m    362\u001b[0m endpoint \u001b[39m=\u001b[39m _LocalEndpoint(EndpointName, EndpointConfigName, Tags, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n\u001b[1;32m    363\u001b[0m LocalSagemakerClient\u001b[39m.\u001b[39m_endpoints[EndpointName] \u001b[39m=\u001b[39m endpoint\n\u001b[0;32m--> 364\u001b[0m endpoint\u001b[39m.\u001b[39;49mserve()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/entities.py:608\u001b[0m, in \u001b[0;36m_LocalEndpoint.serve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mserve(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprimary_container[\u001b[39m\"\u001b[39m\u001b[39mModelDataUrl\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprimary_container[\u001b[39m\"\u001b[39m\u001b[39mEnvironment\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    605\u001b[0m )\n\u001b[1;32m    607\u001b[0m serving_port \u001b[39m=\u001b[39m get_config_value(\u001b[39m\"\u001b[39m\u001b[39mlocal.serving_port\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_session\u001b[39m.\u001b[39mconfig) \u001b[39mor\u001b[39;00m \u001b[39m8080\u001b[39m\n\u001b[0;32m--> 608\u001b[0m _wait_for_serving_container(serving_port)\n\u001b[1;32m    609\u001b[0m \u001b[39m# the container is running and it passed the healthcheck status is now InService\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m _LocalEndpoint\u001b[39m.\u001b[39m_IN_SERVICE\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/local/entities.py:949\u001b[0m, in \u001b[0;36m_wait_for_serving_container\u001b[0;34m(serving_port)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 949\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "model_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=\"us-east-1\",\n",
    "    version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    ")\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name=model_name,\n",
    "    role=role,\n",
    "    entry_point=model_entry_point,\n",
    "    model_data=model_data,\n",
    "    image_uri=model_image_uri,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"local_gpu\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "sagemaker_session.settings = sagemaker.session_settings.SessionSettings(\n",
    "    local_download_dir = local_temp_folder_path\n",
    ")\n",
    "\n",
    "role = \"arn:aws:iam::138140302683:role/service-role/AmazonSageMaker-ExecutionRole-20230522T162566\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create the model\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    role=role,\n",
    "    entry_point=model_entry_point,\n",
    "    model_data=model_data,\n",
    "    image_uri=model_image_uri,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.model.ModelPackage at 0x7f6bf15d3af0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: register the model\n",
    "\n",
    "model.register(\n",
    "    model_package_group_name=\"ai-module-group-name\",\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g4dn.xlarge\"],\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# step 3: create endpoint\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def base64_to_audio_file(base64_string: str, audio_file_path: str) -> None:\n",
    "    \"\"\"Converts a base64-encoded string in an audio file\"\"\"\n",
    "\n",
    "    with open(audio_file_path, \"wb\") as audio_file:\n",
    "        audio_file.write(base64.b64decode((base64_string)))\n",
    "\n",
    "input_data = {\n",
    "    \"prompt\": \"berghain acid techno\",\n",
    "    \"duration\": 1,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"cfg_coefficient\": 3.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(data=input_data)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "base64_audio = response[\"result\"][\"prediction\"]\n",
    "base64_to_audio_file(base64_audio, \"predictor_response.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = \"endpoint-image-generation-0001-dev\"\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(input_data),\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint \n",
    "# NOTE: this doesn't delete the model in the s3 bucket, nor it deletes the model from\n",
    "# model registry\n",
    "\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
